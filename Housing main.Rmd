---
title: "Housing main"
author: "James Owens"
date: "2023-11-15"
output: html_document
---

start of project

```{r setup, include=FALSE}
# Load necessary libraries
library(tidyverse)

# Load your data
data <- read_csv("C:/Users/james/OneDrive/Documents/Housing/FullertonHousing (2).csv")

# Fit a linear regression model
model <- lm(PRICE ~ BEDS + BATHS + SQUARE_FEET + LOT_SIZE + YEAR_BUILT + DAYS_ON_MARKET + HOA, data = data)

# View summary of the model
summary(model)

# Generate predictions
predictions <- predict(model, data)

# Evaluate the model
# Compute RMSE
rmse <- sqrt(mean((data$PRICE - predictions)^2, na.rm = TRUE))

# Print RMSE
print(rmse)

# Compute R-squared
rsq <- summary(model)$r.squared

# Print R-squared
print(rsq)



```


```{r}
# Load necessary libraries
library(glmnet)
library(mice)
library(tidyverse)


# Impute missing values using the mice package (using mean imputation for simplicity)
data_imputed <- data
for (col in colnames(data_imputed)) {
  data_imputed[[col]][is.na(data_imputed[[col]])] <- mean(data_imputed[[col]], na.rm = TRUE)
}

# Prepare the data for glmnet (requires a matrix of predictors and a vector of the response)
predictors_imputed <- as.matrix(data_imputed[, c("BEDS", "BATHS", "SQUARE_FEET", "LOT_SIZE", "YEAR_BUILT", "DAYS_ON_MARKET", "HOA")])
response_imputed <- data_imputed$PRICE

# Fit a ridge regression model
ridge_model <- glmnet(predictors_imputed, response_imputed, alpha = 0)

# Select the best lambda using cross-validation
cv_ridge <- cv.glmnet(predictors_imputed, response_imputed, alpha = 0)
best_lambda <- cv_ridge$lambda.min

# Refit the model with the best lambda
final_ridge_model <- glmnet(predictors_imputed, response_imputed, alpha = 0, lambda = best_lambda)

# Generate predictions
ridge_predictions <- predict(final_ridge_model, s = best_lambda, newx = predictors_imputed)

# Compute RMSE for ridge model
ridge_rmse <- sqrt(mean((response_imputed - ridge_predictions)^2, na.rm = TRUE))
print(ridge_rmse)

# Print the best lambda value
print(best_lambda)


```


```{r plot_chunk, echo=TRUE, fig.cap="My Plot"}
# Load necessary libraries for plotting
library(ggplot2)

# a. Scatter Plot with Regression Line for a significant predictor (e.g., SQUARE_FEET)
ggplot(data, aes(x = SQUARE_FEET, y = PRICE)) +
  geom_point() +
  geom_smooth(method = "lm", col = "blue") +
  theme_minimal() +
  labs(title = "Price vs. Square Feet", x = "Square Feet", y = "Price")

# b. RMSE Comparison Plot
rmse_data <- data.frame(
  Model = c("Linear", "Ridge"),
  RMSE = c(rmse, ridge_rmse)
)

ggplot(rmse_data, aes(x = Model, y = RMSE, fill = Model)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  theme_minimal() +
  labs(title = "RMSE Comparison", x = "Model", y = "RMSE")

# c. Coefficient Plot for Ridge Regression
# Corrected approach to handle coefficient data
ridge_coeffs_matrix <- as.matrix(coef(final_ridge_model, s = best_lambda))
ridge_coeffs_df <- as.data.frame(t(ridge_coeffs_matrix))  # Transpose and convert to data frame
names(ridge_coeffs_df) <- c("Coefficient")
ridge_coeffs_df$Variable <- row.names(ridge_coeffs_df)  # Add a column for variable names
ridge_coeffs_df <- ridge_coeffs_df[-1, ]  # Exclude the intercept

# Create the Coefficient Plot
ggplot(ridge_coeffs_df, aes(x = reorder(Variable, Coefficient), y = Coefficient)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  theme_minimal() +
  labs(title = "Coefficients in Ridge Regression Model", x = "Variable", y = "Coefficient")


```

